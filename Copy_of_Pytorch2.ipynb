{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/get-started/locally/"
      ],
      "metadata": {
        "id": "2HdkQWGSpgGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZgZr2LWc8hh",
        "outputId": "f405c75f-02fb-4bcc-985d-fdf86039b413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "EtQtgQTLp4ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial L}{\\partial w}$ &nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp; loss.backward()"
      ],
      "metadata": {
        "id": "AAJ6FJzHB33O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial L}{\\partial w}$ &nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp; W.grad"
      ],
      "metadata": {
        "id": "_Wd4prv2AK68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W = W - (learning_rate * W.grad) &nbsp;&nbsp;&nbsp; :&nbsp;&nbsp;&nbsp; optimizer.step()"
      ],
      "metadata": {
        "id": "BYonLGvyAlF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now more Pytorch way"
      ],
      "metadata": {
        "id": "yXIwAjtpzgO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneParameterModal(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # A learnable parameter tensor\n",
        "        self.my_weight = nn.Parameter(torch.tensor([0.2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  y = my_weight @ x\n",
        "        # torch.matmul(x, self.my_weight)\n",
        "        return self.my_weight @ x\n",
        "\n",
        "# Initialize the model\n",
        "model = OneParameterModal()\n",
        "\n",
        "# Accessing parameters tracked by the module\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}, Shape: {param.shape}, Requires grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl31GdP3xlDZ",
        "outputId": "f8147eec-fd97-4d54-9671-796096987d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: my_weight, Shape: torch.Size([1]), Requires grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Setup ---\n",
        "# Define a Loss Function (Mean Squared Error is common for regression)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define an Optimizer (Stochastic Gradient Descent is simple and effective)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # Learning Rate (lr) set to 0.1\n",
        "# It will update only the parameters in model.parameters()"
      ],
      "metadata": {
        "id": "cgMgbIcy4X26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ideal input (x) and the target output (target_y)\n",
        "x = torch.tensor([1.0])\n",
        "target_y = torch.tensor([1.0]) # We want the output to be 1.0\n",
        "\n",
        "num_of_epochs = 5"
      ],
      "metadata": {
        "id": "3ie5YT-46V-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. The Training Loop (e.g., 5 epochs) ---\n",
        "print(\"Starting Training...\")\n",
        "print(f\"Initial Weight: {model.my_weight.item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# for i in range(5):\n",
        "for epoch in range(num_of_epochs):\n",
        "    # Zero the gradients from the previous step (Equivalent to W.grad.zero_())\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 1. Forward Pass: Get the prediction (y_pred)\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # 2. Calculate Loss (Now using MSE)\n",
        "    loss = criterion(y_pred, target_y)\n",
        "\n",
        "    # 3. Backward Pass: Calculate the gradient of the loss with respect to the weight\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Optimization Step: Update parameter (weight) (The Manual Gradient Descent Step)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.6f}, New Weight = {model.my_weight.item():.4f}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Final Predicted y: {model(x).item():.4f}\")\n",
        "print(f\"Final Target y:    {target_y.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yoQx4e666nj",
        "outputId": "946b4ea2-a260-4f2d-ad43-8647ec5e316c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Initial Weight: 0.2000\n",
            "------------------------------\n",
            "Epoch 1: Loss = 0.640000, New Weight = 0.3600\n",
            "Epoch 2: Loss = 0.409600, New Weight = 0.4880\n",
            "Epoch 3: Loss = 0.262144, New Weight = 0.5904\n",
            "Epoch 4: Loss = 0.167772, New Weight = 0.6723\n",
            "Epoch 5: Loss = 0.107374, New Weight = 0.7379\n",
            "------------------------------\n",
            "Final Predicted y: 0.7379\n",
            "Final Target y:    1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EvqxGl2AjGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bias"
      ],
      "metadata": {
        "id": "8R0Bcnya8LQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With Bias\n",
        "# --- 1. Define Data and Hyperparameters ---\n",
        "x_data = torch.tensor([[1.0]])\n",
        "y_true = torch.tensor([[1.0]])\n",
        "# --- Model Parameter (W) ---\n",
        "W = torch.tensor([[0.2]], requires_grad=True)\n",
        "b = torch.tensor([[0.2]], requires_grad=True)\n",
        "# --- Hyperparameters\n",
        "learning_rate = 0.1\n",
        "num_epochs = 200\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ### STEP 1: Forward Pass (Calculate Prediction) ###\n",
        "    y_pred = (W @ x_data) + b\n",
        "\n",
        "    ### STEP 2: Calculate Loss ###\n",
        "    loss = (y_true - y_pred).pow(2)\n",
        "\n",
        "    ### STEP 3: Backward Pass (Calculate Gradients) ###\n",
        "    loss.backward()\n",
        "\n",
        "    ### STEP 4: Update Parameters (The Manual Gradient Descent Step) ###\n",
        "    with torch.no_grad():\n",
        "        # W = W - (learning_rate * W.grad)\n",
        "        W -= (learning_rate * W.grad)\n",
        "        b -= (learning_rate * b.grad)\n",
        "\n",
        "    ### STEP 5: Reset means Zero the Gradients for the next iteration###\n",
        "    W.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # --- Print and Break Condition ---\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.3f}, Weight: {W.item():.4f}, bias: {W.item():.4f}, y: {y_pred}\")\n",
        "\n",
        "    # Break condition requested by the user\n",
        "    if loss.item() < 0.0001:\n",
        "        print(f\"\\nBreak condition met: Loss ({loss.item():.6f}) < 0.1\")\n",
        "        break\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Final optimized weight (W): {W.item():.4f}, bias (b): {b.item():.4f}\")"
      ],
      "metadata": {
        "id": "CJ_g3BMVpgfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78cc8b0d-1d05-4eab-e2fa-88aba828faed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 0.360, Weight: 0.3200, bias: 0.3200, y: tensor([[0.4000]], grad_fn=<AddBackward0>)\n",
            "Epoch [2/200], Loss: 0.130, Weight: 0.3920, bias: 0.3920, y: tensor([[0.6400]], grad_fn=<AddBackward0>)\n",
            "Epoch [3/200], Loss: 0.047, Weight: 0.4352, bias: 0.4352, y: tensor([[0.7840]], grad_fn=<AddBackward0>)\n",
            "Epoch [4/200], Loss: 0.017, Weight: 0.4611, bias: 0.4611, y: tensor([[0.8704]], grad_fn=<AddBackward0>)\n",
            "Epoch [5/200], Loss: 0.006, Weight: 0.4767, bias: 0.4767, y: tensor([[0.9222]], grad_fn=<AddBackward0>)\n",
            "Epoch [6/200], Loss: 0.002, Weight: 0.4860, bias: 0.4860, y: tensor([[0.9533]], grad_fn=<AddBackward0>)\n",
            "Epoch [7/200], Loss: 0.001, Weight: 0.4916, bias: 0.4916, y: tensor([[0.9720]], grad_fn=<AddBackward0>)\n",
            "Epoch [8/200], Loss: 0.000, Weight: 0.4950, bias: 0.4950, y: tensor([[0.9832]], grad_fn=<AddBackward0>)\n",
            "Epoch [9/200], Loss: 0.000, Weight: 0.4970, bias: 0.4970, y: tensor([[0.9899]], grad_fn=<AddBackward0>)\n",
            "Epoch [10/200], Loss: 0.000, Weight: 0.4982, bias: 0.4982, y: tensor([[0.9940]], grad_fn=<AddBackward0>)\n",
            "\n",
            "Break condition met: Loss (0.000037) < 0.1\n",
            "---------------------------------------\n",
            "Final optimized weight (W): 0.4982, bias (b): 0.4982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Pytorch way"
      ],
      "metadata": {
        "id": "C3l4dH-vveTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. Define Data and Hyperparameters ---\n",
        "x_data = torch.tensor([[1.0]])\n",
        "y_true = torch.tensor([[1.0]])\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# --- 2. Define the Model Class (Replaces W and b tensors) ---\n",
        "# ------------------------------------------------------------------\n",
        "class SimpleLinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # nn.Linear automatically creates W and b tensors with requires_grad=True\n",
        "        # It takes (taking input_features, producing output_features)\n",
        "        # self.linear = nn.Linear(1, 1)\n",
        "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "        # # Set initial weights and bias to match your original problem\n",
        "        self.linear.weight.data = torch.tensor([[0.2]])\n",
        "        self.linear.bias.data = torch.tensor([0.2])\n",
        "\n",
        "        # Inspect the layer's parameters (automatically created)\n",
        "        print(f\"\\nWeight shape: {self.linear.weight.shape}\")\n",
        "        print(f\"Bias shape: {self.linear.bias.shape}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "      #  y = weight @ x + bias\n",
        "        return self.linear(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleLinearModel()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# --- 3. Define Loss Function and Optimizer ---\n",
        "# ------------------------------------------------------------------\n",
        "# Standard Squared Loss (MSE)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Standard Stochastic Gradient Descent (SGD) optimizer\n",
        "# It takes the model's parameters and the learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# --- 4. Training Loop (Simplified) ---\n",
        "# ------------------------------------------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ### STEP 1: Reset Gradients (Equivalent to W.grad.zero_()) ###\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ### STEP 2: Forward Pass (Calculate Prediction) ###\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    ### STEP 3: Calculate Loss (Now using MSE) ###\n",
        "    loss = criterion(y_pred, y_true)\n",
        "\n",
        "    ### STEP 4: Backward Pass (Calculate Gradients) ###\n",
        "    loss.backward()\n",
        "\n",
        "    ### STEP 5: Update Parameters (The Manual Gradient Descent Step) ###\n",
        "    # This automatically updates W and b using the calculated gradients and learning_rate\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}, W: {W:.4f}, b: {b:.4f}, y_pred: {y_pred.item():.4f}\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(f\"Final optimized weight (W): {W:.4f}, bias (b): {b:.4f}\")\n",
        "print(f\"Final Prediction (W@x + b): {W*1.0 + b:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "wC2g651VvRqV",
        "outputId": "88f24aa3-ccc9-4f7e-bb9c-c4bcbc7d5c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weight shape: torch.Size([1, 1])\n",
            "Bias shape: torch.Size([1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to Tensor.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3053827024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}, W: {W:.4f}, b: {b:.4f}, y_pred: {y_pred.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;31m# requires gradients to a python number. It is ok for formatting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9myVQWMF5QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Linear"
      ],
      "metadata": {
        "id": "t9q7z0qsufiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Expecting input features of size 20, producing output features of size 30\n",
        "linear_layer = nn.Linear(in_features=4, out_features=2)\n",
        "\n",
        "# Inspect the layer's parameters (automatically created)\n",
        "print(f\"\\nWeight shape: {linear_layer.weight.shape}\")\n",
        "print(f\"Bias shape: {linear_layer.bias.shape}\")"
      ],
      "metadata": {
        "id": "OjMsbG7-F5bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JD08iMefuQND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Model with Activation"
      ],
      "metadata": {
        "id": "lnwE0RB9xfo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Define the Model Class\n",
        "class SimpleReluModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleReluModel, self).__init__()\n",
        "        # Linear layer: 2 input features, 1 output feature\n",
        "        self.linear = nn.Linear(in_features=2, out_features=1)\n",
        "        # The activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Apply linear transformation (W*x + b)\n",
        "        x = self.linear(x)\n",
        "        # 2. Apply ReLU activation (max(0, output))\n",
        "        output = self.relu(x)\n",
        "        return output\n",
        "\n",
        "# 2. Instantiate the Model\n",
        "model = SimpleReluModel()\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# 3. Create Example Input Data\n",
        "# Shape: (batch_size, num_features) -> (1, 2)\n",
        "input_data = torch.randn(1, 2)\n",
        "print(\"\\nExample Input Data:\")\n",
        "print(input_data)\n",
        "\n",
        "# 4. Pass the Input through the Model\n",
        "output = model(input_data)\n",
        "\n",
        "# 5. Print the Output\n",
        "print(\"\\nModel Output (Shape: [1, 1]):\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "4iuV-zw1gBe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tV1H2k8EyM_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It starts with hidden layer or input layer?"
      ],
      "metadata": {
        "id": "0tIlTMIKyOUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Universal Approximation Theorem"
      ],
      "metadata": {
        "id": "_Ksx07zLu6oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://alexlenail.me/NN-SVG/"
      ],
      "metadata": {
        "id": "_FASA5OLwbfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X = torch.linspace(0, 1, 100).unsqueeze(1)\n",
        "# X = torch.linspace(0, 1, 100)\n",
        "# X"
      ],
      "metadata": {
        "id": "qtEd6lH3v33k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Define the Function to Approximate (f(x) = sin(2*pi*x)) ---\n",
        "def target_function(x):\n",
        "    return torch.sin(2 * torch.pi * x)\n",
        "\n",
        "# --- 2. Create Training Data ---\n",
        "# 100 data points in the domain [0, 1]\n",
        "N_SAMPLES = 100\n",
        "X = torch.linspace(0, 1, N_SAMPLES).unsqueeze(1) # Input: shape (100, 1) -> 1 feature\n",
        "Y = target_function(X)                          # Target: shape (100, 1)\n",
        "\n",
        "# --- 3. Define the Universal Approximator Network ---\n",
        "class UniversalApproximator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(UniversalApproximator, self).__init__()\n",
        "        # Single hidden layer is sufficient for the UAT\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use ReLU as the non-polynomial activation function\n",
        "        x = torch.relu(self.hidden(x))\n",
        "        x = self.output(x) # Linear output layer for regression\n",
        "        return x\n",
        "\n",
        "# --- 4. Instantiate Model, Loss, and Optimizer ---\n",
        "input_dim = 1      # n=1 feature (x)\n",
        "hidden_dim = 50    # Arbitrary number of hidden neurons (the 'width')\n",
        "output_dim = 1     # m=1 output (f(x))\n",
        "\n",
        "model = UniversalApproximator(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "NUM_EPOCHS = 2000\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Forward pass\n",
        "    Y_pred = model(X)\n",
        "    loss = criterion(Y_pred, Y)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "bi84RAcbW5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Plotting the Approximation ---\n",
        "with torch.no_grad():\n",
        "    Y_approx = model(X).squeeze().numpy()\n",
        "\n",
        "X_np = X.squeeze().numpy()\n",
        "Y_np = Y.squeeze().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(X_np, Y_np, label='Target Function (sin(2Ï€x))', color='blue', linewidth=2)\n",
        "plt.plot(X_np, Y_approx, label='NN Approximation', color='red', linestyle='--', linewidth=2)\n",
        "plt.title('Universal Approximation Theorem Demonstration (1D)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k2_sG84QvREt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Sequential"
      ],
      "metadata": {
        "id": "SuGjuFzgvjGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "x = torch.linspace(-3.14, 3.14, 400).unsqueeze(1)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(3000):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Plot results\n",
        "plt.plot(x.detach(), y, label='True sin(x)')\n",
        "plt.plot(x.detach(), y_pred.detach(), label='Neural net approximation')\n",
        "plt.legend()\n",
        "plt.title('Universal Approximation with ReLU (PyTorch)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vXXAEeIYUiac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFAbSWfRmaC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}